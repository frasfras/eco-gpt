{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koiUuDc_rNUB",
        "outputId": "84806f30-f974-4d66-ca7d-35913b772621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Install dependencies\n",
        "!pip -q install streamlit pyngrok PyPDF2 sentence_transformers faiss-cpu transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxr1xsezBlq7"
      },
      "outputs": [],
      "source": [
        "pip -q install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsmfKXANePe8"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch\n",
        "!pip install -q transformers triton==3.4 kernels\n",
        "!pip uninstall -q torchvision torchaudio -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKXeCROuf87D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_WiaKNPfCIG"
      },
      "outputs": [],
      "source": [
        "#login\n",
        "from huggingface_hub import login\n",
        "#Set your HUGGINGFACE auth token\n",
        "login(\"your_token_here\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kj7Kg0tW0OnI",
        "outputId": "44fb43a6-a02e-494b-dc39-e28077c6d111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-oss-20b'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 67 (delta 27), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (67/67), 35.88 KiB | 1.49 MiB/s, done.\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 2 ‚Äì clone GPT‚ÄëOSS (20‚ÄØB) with Git‚ÄëLFS\n",
        "!git clone https://huggingface.co/openai/gpt-oss-20b\n",
        "# If you want the smaller 7‚ÄØB version, replace the URL above with gpt-oss-7b\n",
        "# (or whichever GGUF you have).\n",
        "# The clone will download ~70‚ÄØGB; use a Pro+ runtime or a persistent drive!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2nFLNCruReU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo6JBPFD99mk",
        "outputId": "478800c0-a2d4-49d8-edce-7158cfd4e17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import hashlib\n",
        "\n",
        "\n",
        "# --- Load model + tokenizer ---\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_path = \"/content/gpt-oss-20b\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "       model_path,\n",
        "       torch_dtype=\"auto\",\n",
        "       device_map=\"auto\",   # auto = GPU if available, fallback CPU\n",
        "   )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "# --------------------------\n",
        "# Load embedding model\n",
        "# --------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# --------------------------\n",
        "# PDF extraction\n",
        "# --------------------------\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# --------------------------\n",
        "# Cached PDF embeddings\n",
        "# --------------------------\n",
        "# @st.cache_resource(show_spinner=False)\n",
        "# def get_pdf_index(file_bytes):\n",
        "#     \"\"\"Cache embeddings per unique PDF\"\"\"\n",
        "#     file_hash = hashlib.md5(file_bytes).hexdigest()\n",
        "#     text = extract_text_from_pdf(file_bytes)\n",
        "#     pdf_chunks = chunk_text(text)\n",
        "#     vectors = embedder.encode(pdf_chunks, convert_to_numpy=True)\n",
        "#     index = faiss.IndexFlatL2(vectors.shape[1])\n",
        "#     index.add(vectors)\n",
        "#     return pdf_chunks, index\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_pdf_index(file_bytes):\n",
        "    \"\"\"Cache embeddings per unique PDF\"\"\"\n",
        "    file_hash = hashlib.md5(file_bytes).hexdigest()\n",
        "\n",
        "    # Read PDF from bytes\n",
        "    import io\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "\n",
        "    pdf_chunks = chunk_text(text)\n",
        "    vectors = embedder.encode(pdf_chunks, convert_to_numpy=True)\n",
        "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
        "    index.add(vectors)\n",
        "    return pdf_chunks, index\n",
        "\n",
        "# --------------------------\n",
        "# Retrieval + prompt\n",
        "# --------------------------\n",
        "def make_prompt(question, df_context, pdf_chunks, pdf_index):\n",
        "    pdf_context = \"\"\n",
        "    if pdf_chunks and pdf_index:\n",
        "        q_vec = embedder.encode([question], convert_to_numpy=True)\n",
        "        D, I = pdf_index.search(q_vec, k=3)\n",
        "        pdf_context = \"\\n\".join([pdf_chunks[i] for i in I[0]])\n",
        "\n",
        "    csv_context = \"\"\n",
        "    if df_context is not None:\n",
        "        csv_context = \"\\n\".join(\n",
        "            f\"{row['timestamp']} | {row['sensor_id']} | {row['event_type']} | {row['payload']}\"\n",
        "            for _, row in df_context.iterrows()\n",
        "        )\n",
        "\n",
        "    return f\"\"\"\n",
        "You are Eco-GPT, a scientific assistant.\n",
        "\n",
        "Logs:\n",
        "{csv_context}\n",
        "\n",
        "Scientific Notes:\n",
        "{pdf_context}\n",
        "\n",
        "Question: {question}\n",
        "Answer (be accurate, concise, and eco friendly):\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------\n",
        "# Streamlit UI\n",
        "# --------------------------\n",
        "st.set_page_config(page_title=\"Eco-GPT\", page_icon=\"üå±\")\n",
        "st.title(\"üå± Eco-GPT: Scientific Q&A\")\n",
        "st.caption(\"Upload CSV logs + scientific PDFs, then ask natural questions.\")\n",
        "\n",
        "df = None\n",
        "pdf_chunks, pdf_index = None, None\n",
        "\n",
        "# File uploaders\n",
        "uploaded_csv = st.file_uploader(\"Upload CSV logs\", type=[\"csv\"])\n",
        "uploaded_pdf = st.file_uploader(\"Upload scientific PDF\", type=[\"pdf\"])\n",
        "\n",
        "# Process CSV\n",
        "if uploaded_csv:\n",
        "    df = pd.read_csv(uploaded_csv)\n",
        "    st.subheader(\"üìú CSV Preview\")\n",
        "    st.dataframe(df.head(20))\n",
        "\n",
        "# Process PDF\n",
        "if uploaded_pdf:\n",
        "    with st.spinner(\"Extracting & indexing PDF... (cached after first run)\"):\n",
        "        file_bytes = uploaded_pdf.getvalue()  # ‚úÖ convert to bytes\n",
        "        pdf_chunks, pdf_index = get_pdf_index(file_bytes)\n",
        "    st.success(\"PDF indexed successfully ‚úÖ\")\n",
        "\n",
        "# Question input\n",
        "user_input = st.text_area(\"Ask a question:\", \"What animals were near stream #3?\")\n",
        "if st.button(\"Ask Eco-GPT\"):\n",
        "    if user_input.strip():\n",
        "        with st.spinner(\"Thinking in riddles...\"):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n",
        "                {\"role\": \"user\", \"content\": user_input},\n",
        "            ]\n",
        "\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_dict=True,\n",
        "            ).to(model.device)\n",
        "\n",
        "            generated = model.generate(**inputs, max_new_tokens=200)\n",
        "            output_text = tokenizer.decode(\n",
        "                generated[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        st.subheader(\"üåç Eco-GPT Answer\")\n",
        "        st.write(output_text)\n",
        "    else:\n",
        "        st.warning(\"Please type a message first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0c4oqvXrcBi"
      },
      "outputs": [],
      "source": [
        "import os, subprocess, time, threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token (get free token from ngrok.com)\n",
        "\n",
        "# (Optional) Paste your ngrok auth token here for a more stable tunnel\n",
        "NGROK_AUTH_TOKEN = \"\"  # e.g. \"2Qx...your_token...Abc\"\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "PORT = 8501\n",
        "\n",
        "# Close any existing tunnels to avoid duplicates\n",
        "for t in ngrok.get_tunnels():\n",
        "    try:\n",
        "        ngrok.disconnect(t.public_url)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Start Streamlit in the background\n",
        "cmd = [\n",
        "    \"streamlit\", \"run\", \"app.py\",\n",
        "    \"--server.port\", str(PORT),\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.headless\", \"true\",\n",
        "]\n",
        "log_path = \"/content/streamlit.log\"\n",
        "log_file = open(log_path, \"w\")\n",
        "proc = subprocess.Popen(cmd, stdout=log_file, stderr=log_file, text=True)\n",
        "\n",
        "# Give Streamlit a moment to boot\n",
        "time.sleep(3)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "tunnel = ngrok.connect(addr=PORT, proto=\"http\")\n",
        "public_url = tunnel.public_url\n",
        "print(\"Your app is live at:\", public_url)\n",
        "\n",
        "# Live-tail the Streamlit logs so you can see when it's ready\n",
        "def tail_logs(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        f.seek(0, os.SEEK_END)\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if line:\n",
        "                print(line, end=\"\")\n",
        "            else:\n",
        "                time.sleep(0.5)\n",
        "\n",
        "threading.Thread(target=tail_logs, args=(log_path,), daemon=True).start()\n",
        "\n",
        "# Keep the cell alive so the tunnel stays open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVlJc3rDKqvD",
        "outputId": "3cce650a-c361-411a-a8b3-1cc6f86dd39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing sample_logs.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile sample_logs.csv\n",
        "timestamp,sensor_id,event_type,payload,metadata\n",
        "2024-08-01T06:14:00,cam_trap_01,image,Jaguar observed near stream #3,\"{\"lat\": -3.45, \"lon\": -62.78}\"\n",
        "2024-08-01T06:15:00,mic_01,audio,Cicada chorus intensity: high,\"{\"temp_c\": 27.3}\"\n",
        "2024-08-01T06:20:00,env_01,temperature,Air temperature 28¬∞C,\"{\"humidity\":71}\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
