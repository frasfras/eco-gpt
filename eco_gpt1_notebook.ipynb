{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koiUuDc_rNUB",
        "outputId": "77748589-d227-429a-fdd2-310ab041ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Install dependencies\n",
        "!pip -q install streamlit pyngrok PyPDF2 sentence_transformers faiss-cpu transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsmfKXANePe8"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch\n",
        "!pip install -q transformers triton==3.4 kernels\n",
        "!pip uninstall -q torchvision torchaudio -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKXeCROuf87D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2nFLNCruReU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwQu8-q7Xmp_",
        "outputId": "b9d56aa6-690d-470b-e02f-067aea52e95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import faiss\n",
        "import io\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import openai\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Setup API client\n",
        "# --------------------------\n",
        "#new_var = os.environ.get(\"SYNTHETIC_API_KEY\")\n",
        "client = openai.OpenAI(\n",
        "  # api_key=new_var,\n",
        "    api_key=\"syn_6042570bd16bbf5f4a847147884c9a80\",\n",
        "    base_url=\"https://api.synthetic.new/v1\",\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Load embedding model\n",
        "# --------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# --------------------------\n",
        "# PDF handling\n",
        "# --------------------------\n",
        "def extract_text_from_pdf(file_bytes):\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# --------------------------\n",
        "# Cached PDF embeddings\n",
        "# --------------------------\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_pdf_index(file_bytes):\n",
        "    file_hash = hashlib.md5(file_bytes).hexdigest()\n",
        "    text = extract_text_from_pdf(file_bytes)\n",
        "    pdf_chunks = chunk_text(text)\n",
        "    vectors = embedder.encode(pdf_chunks, convert_to_numpy=True)\n",
        "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
        "    index.add(vectors)\n",
        "    return pdf_chunks, index\n",
        "\n",
        "# --------------------------\n",
        "# Retrieval + prompt building\n",
        "# --------------------------\n",
        "def make_prompt(question, df_context, pdf_chunks, pdf_index):\n",
        "    pdf_context = \"\"\n",
        "    if pdf_chunks and pdf_index:\n",
        "        q_vec = embedder.encode([question], convert_to_numpy=True)\n",
        "        D, I = pdf_index.search(q_vec, k=3)\n",
        "        pdf_context = \"\\n\".join([pdf_chunks[i] for i in I[0]])\n",
        "\n",
        "    csv_context = \"\"\n",
        "    if df_context is not None:\n",
        "        csv_context = \"\\n\".join(\n",
        "            f\"{row['timestamp']} | {row['sensor_id']} | {row['event_type']} | {row['payload']}\"\n",
        "            for _, row in df_context.iterrows()\n",
        "        )\n",
        "\n",
        "    return f\"\"\"\n",
        "You are Eco-GPT, a scientific assistant.\n",
        "\n",
        "Logs:\n",
        "{csv_context}\n",
        "\n",
        "Scientific Notes:\n",
        "{pdf_context}\n",
        "\n",
        "Question: {question}\n",
        "Answer (be accurate, concise, and eco-friendly):\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------\n",
        "# Streamlit UI\n",
        "# --------------------------\n",
        "st.set_page_config(page_title=\"Eco-GPT\", page_icon=\"ğŸŒ±\")\n",
        "st.title(\"ğŸŒ± Eco-GPT: Scientific Q&A\")\n",
        "st.caption(\"Upload CSV logs + scientific PDFs, then ask natural questions. Powered by GPT-OSS via Synthetic API.\")\n",
        "\n",
        "df = None\n",
        "pdf_chunks, pdf_index = None, None\n",
        "\n",
        "uploaded_csv = st.file_uploader(\"Upload CSV logs\", type=[\"csv\"])\n",
        "uploaded_pdf = st.file_uploader(\"Upload scientific PDF\", type=[\"pdf\"])\n",
        "\n",
        "if uploaded_csv:\n",
        "    df = pd.read_csv(uploaded_csv)\n",
        "    st.subheader(\"ğŸ“œ CSV Preview\")\n",
        "    st.dataframe(df.head(20))\n",
        "\n",
        "if uploaded_pdf:\n",
        "    with st.spinner(\"Extracting & indexing PDF... (cached after first run)\"):\n",
        "        file_bytes = uploaded_pdf.getvalue()\n",
        "        pdf_chunks, pdf_index = get_pdf_index(file_bytes)\n",
        "    st.success(\"PDF indexed successfully âœ…\")\n",
        "\n",
        "user_input = st.text_area(\"Ask a question:\", \"What animals were near stream #3?\")\n",
        "\n",
        "if st.button(\"Ask Eco-GPT\"):\n",
        "    if user_input.strip():\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            prompt = make_prompt(user_input, df, pdf_chunks, pdf_index)\n",
        "\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"hf:openai/gpt-oss-120b\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are Eco-GPT, a scientific assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            answer = completion.choices[0].message.content\n",
        "\n",
        "        st.subheader(\"ğŸŒ Eco-GPT Answer\")\n",
        "        st.write(answer)\n",
        "    else:\n",
        "        st.warning(\"Please type a question first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0c4oqvXrcBi",
        "outputId": "be8b075f-805e-440c-cd85-c31b27566a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your app is live at: https://6f975e12a24a.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "#Cell 4 Run in Google Colab with ngrok\"\n",
        "import os, subprocess, time, threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token (get free token from ngrok.com)\n",
        "\n",
        "# (Optional) Paste your ngrok auth token here for a more stable tunnel\n",
        "NGROK_AUTH_TOKEN = \"328AAEi1ftAKNJGyCIyCrcfNtwI_3hsW7NkpZNsnanimRsTin\"  # e.g. \"2Qx...your_token...Abc\"\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "PORT = 8501\n",
        "\n",
        "# Close any existing tunnels to avoid duplicates\n",
        "for t in ngrok.get_tunnels():\n",
        "    try:\n",
        "        ngrok.disconnect(t.public_url)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Start Streamlit in the background\n",
        "cmd = [\n",
        "    \"streamlit\", \"run\", \"app.py\",\n",
        "    \"--server.port\", str(PORT),\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.headless\", \"true\",\n",
        "]\n",
        "log_path = \"/content/streamlit.log\"\n",
        "log_file = open(log_path, \"w\")\n",
        "proc = subprocess.Popen(cmd, stdout=log_file, stderr=log_file, text=True)\n",
        "\n",
        "# Give Streamlit a moment to boot\n",
        "time.sleep(3)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "tunnel = ngrok.connect(addr=PORT, proto=\"http\")\n",
        "public_url = tunnel.public_url\n",
        "print(\"Your app is live at:\", public_url)\n",
        "\n",
        "# Live-tail the Streamlit logs so you can see when it's ready\n",
        "def tail_logs(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        f.seek(0, os.SEEK_END)\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if line:\n",
        "                print(line, end=\"\")\n",
        "            else:\n",
        "                time.sleep(0.5)\n",
        "\n",
        "threading.Thread(target=tail_logs, args=(log_path,), daemon=True).start()\n",
        "\n",
        "# Keep the cell alive so the tunnel stays open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVlJc3rDKqvD",
        "outputId": "3cce650a-c361-411a-a8b3-1cc6f86dd39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing sample_logs.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile sample_logs.csv\n",
        "timestamp,sensor_id,event_type,payload,metadata\n",
        "2024-08-01T06:14:00,cam_trap_01,image,Jaguar observed near stream #3,\"{\"lat\": -3.45, \"lon\": -62.78}\"\n",
        "2024-08-01T06:15:00,mic_01,audio,Cicada chorus intensity: high,\"{\"temp_c\": 27.3}\"\n",
        "2024-08-01T06:20:00,env_01,temperature,Air temperature 28Â°C,\"{\"humidity\":71}\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
