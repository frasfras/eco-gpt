{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koiUuDc_rNUB",
        "outputId": "0deeb35c-dbb1-4ef7-8cb4-a1b651d2176e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Install dependencies\n",
        "!pip -q install streamlit pyngrok transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsmfKXANePe8"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch\n",
        "!pip install -q transformers triton==3.4 kernels\n",
        "!pip uninstall -q torchvision torchaudio -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n_WiaKNPfCIG"
      },
      "outputs": [],
      "source": [
        "#login\n",
        "from huggingface_hub import login\n",
        "#Set your HUGGINGFACE auth token\n",
        "login(\"your_token_here\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kj7Kg0tW0OnI",
        "outputId": "44fb43a6-a02e-494b-dc39-e28077c6d111"
      },
      "outputs": [],
      "source": [
        "# Colab cell 2 ‚Äì clone GPT‚ÄëOSS (20‚ÄØB) with Git‚ÄëLFS\n",
        "!git clone https://huggingface.co/openai/gpt-oss-20b\n",
        "# If you want the smaller 7‚ÄØB version, replace the URL above with gpt-oss-7b\n",
        "# (or whichever GGUF you have).\n",
        "# The clone will download ~70‚ÄØGB; use a Pro+ runtime or a persistent drive!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l1Cj-Cwrjw1",
        "outputId": "272d77a3-c97c-4ccd-80fe-71d3e341d0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# --- Load model + tokenizer ---\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_path = \"/content/gpt-oss-20b\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "       model_path,\n",
        "       torch_dtype=\"auto\",\n",
        "       device_map=\"auto\",   # auto = GPU if available, fallback CPU\n",
        "   )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"Chat with GPT OSS 20B\", page_icon=\"ü§ñ\")\n",
        "st.title(\"üåø Eco‚ÄëGPT ‚Äì Open‚ÄëSource Conservation Assistant\")\n",
        "st.caption(\"Ask natural‚Äëlanguage questions about the sensor logs you just uploaded.\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Input box\n",
        "user_input = st.text_area(\"Your message:\", \"What is the weather like in Madrid?\")\n",
        "\n",
        "if st.button(\"Generate Reply\"):\n",
        "    if user_input.strip():\n",
        "        with st.spinner(\"Thinking in riddles...\"):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n",
        "                {\"role\": \"user\", \"content\": user_input},\n",
        "            ]\n",
        "\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_dict=True,\n",
        "            ).to(model.device)\n",
        "\n",
        "            generated = model.generate(**inputs, max_new_tokens=200)\n",
        "            output_text = tokenizer.decode(\n",
        "                generated[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        st.subheader(\"ü§î Model's answer:\")\n",
        "        st.write(output_text)\n",
        "    else:\n",
        "        st.warning(\"Please type a message first.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0c4oqvXrcBi",
        "outputId": "ceda3d5c-44ad-499d-d920-10191ff0b67d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your app is live at: https://d56916a9a3d0.ngrok-free.app\n",
            "2025-09-02 16:14:55.973607: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756829696.002302   36956 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756829696.012785   36956 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756829696.035533   36956 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756829696.035602   36956 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756829696.035610   36956 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756829696.035614   36956 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-02 16:14:56.042441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-02T16:37:25+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-05ed367b-5745-4fdf-9361-fbf621484bc2 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, time, threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# (Optional) Paste your ngrok auth token here for a more stable tunnel\n",
        "NGROK_AUTH_TOKEN = \"\"  # e.g. \"2Qx...your_token...Abc\"\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "PORT = 8501\n",
        "\n",
        "# Close any existing tunnels to avoid duplicates\n",
        "for t in ngrok.get_tunnels():\n",
        "    try:\n",
        "        ngrok.disconnect(t.public_url)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Start Streamlit in the background\n",
        "cmd = [\n",
        "    \"streamlit\", \"run\", \"app.py\",\n",
        "    \"--server.port\", str(PORT),\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.headless\", \"true\",\n",
        "]\n",
        "log_path = \"/content/streamlit.log\"\n",
        "log_file = open(log_path, \"w\")\n",
        "proc = subprocess.Popen(cmd, stdout=log_file, stderr=log_file, text=True)\n",
        "\n",
        "# Give Streamlit a moment to boot\n",
        "time.sleep(3)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "tunnel = ngrok.connect(addr=PORT, proto=\"http\")\n",
        "public_url = tunnel.public_url\n",
        "print(\"Your app is live at:\", public_url)\n",
        "\n",
        "# Live-tail the Streamlit logs so you can see when it's ready\n",
        "def tail_logs(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        f.seek(0, os.SEEK_END)\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if line:\n",
        "                print(line, end=\"\")\n",
        "            else:\n",
        "                time.sleep(0.5)\n",
        "\n",
        "threading.Thread(target=tail_logs, args=(log_path,), daemon=True).start()\n",
        "\n",
        "# Keep the cell alive so the tunnel stays open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVlJc3rDKqvD",
        "outputId": "3cce650a-c361-411a-a8b3-1cc6f86dd39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing sample_logs.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile sample_logs.csv\n",
        "timestamp,sensor_id,event_type,payload,metadata\n",
        "2024-08-01T06:14:00,cam_trap_01,image,Jaguar observed near stream #3,\"{\"lat\": -3.45, \"lon\": -62.78}\"\n",
        "2024-08-01T06:15:00,mic_01,audio,Cicada chorus intensity: high,\"{\"temp_c\": 27.3}\"\n",
        "2024-08-01T06:20:00,env_01,temperature,Air temperature 28¬∞C,\"{\"humidity\":71}\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
